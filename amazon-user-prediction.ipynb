{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Goal\n\nTo create simple model which predicts a users rating for a book given the amount of reviews on it, the price in dollars, year released and genre.\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:48.783286Z","iopub.execute_input":"2023-07-02T20:42:48.783619Z","iopub.status.idle":"2023-07-02T20:42:48.789737Z","shell.execute_reply.started":"2023-07-02T20:42:48.783592Z","shell.execute_reply":"2023-07-02T20:42:48.788713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.Data exploration","metadata":{}},{"cell_type":"code","source":"dataset_path = '/kaggle/input/amazon-top-50-bestselling-books-2009-2019/bestsellers with categories.csv'\namazon_dataset = pd.read_csv(dataset_path)\namazon_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:48.791694Z","iopub.execute_input":"2023-07-02T20:42:48.792413Z","iopub.status.idle":"2023-07-02T20:42:48.816191Z","shell.execute_reply.started":"2023-07-02T20:42:48.792376Z","shell.execute_reply":"2023-07-02T20:42:48.815025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"amazon_dataset.info()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:48.817610Z","iopub.execute_input":"2023-07-02T20:42:48.818054Z","iopub.status.idle":"2023-07-02T20:42:48.833617Z","shell.execute_reply.started":"2023-07-02T20:42:48.818022Z","shell.execute_reply":"2023-07-02T20:42:48.832492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"amazon_dataset.hist(bins=50, figsize=(20,15))","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:48.835100Z","iopub.execute_input":"2023-07-02T20:42:48.835674Z","iopub.status.idle":"2023-07-02T20:42:49.924670Z","shell.execute_reply.started":"2023-07-02T20:42:48.835645Z","shell.execute_reply":"2023-07-02T20:42:49.923693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"User rating, price, and reviews all seem to follow an exponential distribution, with far outliers on the tails. \nThose will all have to be standardized, but for fun lets look at the extreem ends of ditributions first. \n\n","metadata":{}},{"cell_type":"code","source":"amazon_dataset.describe()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:49.926958Z","iopub.execute_input":"2023-07-02T20:42:49.927248Z","iopub.status.idle":"2023-07-02T20:42:49.949157Z","shell.execute_reply.started":"2023-07-02T20:42:49.927224Z","shell.execute_reply":"2023-07-02T20:42:49.948277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The highest amount of reviews a book got in this dataset was 87841, I wonder what book could have lead to everybody discussing it so much.","metadata":{}},{"cell_type":"code","source":"amazon_dataset.loc[amazon_dataset[\"Reviews\"] == 87841]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:49.950453Z","iopub.execute_input":"2023-07-02T20:42:49.950716Z","iopub.status.idle":"2023-07-02T20:42:49.963619Z","shell.execute_reply.started":"2023-07-02T20:42:49.950693Z","shell.execute_reply":"2023-07-02T20:42:49.962075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Somebody more in the fiction space might have guessed this book, doing a quick google searched revealed it used to be the Amazon top best seller and is now getting a movie, it's very popular.\n\nWhats the most expensive book in the list?","metadata":{}},{"cell_type":"code","source":"amazon_dataset.loc[amazon_dataset[\"Price\"] == 105]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:49.965486Z","iopub.execute_input":"2023-07-02T20:42:49.965943Z","iopub.status.idle":"2023-07-02T20:42:49.982980Z","shell.execute_reply.started":"2023-07-02T20:42:49.965907Z","shell.execute_reply":"2023-07-02T20:42:49.981606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The DSM, a manual for psychiatric testing, no wonder it's so expsenive.","metadata":{}},{"cell_type":"code","source":"amazon_dataset.loc[amazon_dataset[\"Price\"] == 0]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:49.984988Z","iopub.execute_input":"2023-07-02T20:42:49.985275Z","iopub.status.idle":"2023-07-02T20:42:50.003378Z","shell.execute_reply.started":"2023-07-02T20:42:49.985249Z","shell.execute_reply":"2023-07-02T20:42:50.002476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the entries where the price is zero there is a clear problem. It looks like there are not only some books with price erroneously entered as zero cost but also duplicates of books in the dataset, that will need to be taken care of, but for now, I'm just going to keep exploring the data.","metadata":{}},{"cell_type":"code","source":"amazon_dataset.loc[amazon_dataset[\"User Rating\"] == 3.3]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:50.004389Z","iopub.execute_input":"2023-07-02T20:42:50.004732Z","iopub.status.idle":"2023-07-02T20:42:50.020140Z","shell.execute_reply.started":"2023-07-02T20:42:50.004701Z","shell.execute_reply":"2023-07-02T20:42:50.018909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"amazon_dataset.loc[amazon_dataset[\"User Rating\"] == 4.9]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:50.021464Z","iopub.execute_input":"2023-07-02T20:42:50.021850Z","iopub.status.idle":"2023-07-02T20:42:50.044980Z","shell.execute_reply.started":"2023-07-02T20:42:50.021816Z","shell.execute_reply":"2023-07-02T20:42:50.043894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Other than the interesting fact of J.K. Rowling being in the best rated in worst rated spots, everything is normal here.","metadata":{}},{"cell_type":"markdown","source":"It also seems the curator of the dataset made sure to pick an equal amount of books from each year. Let’s make sure with one example.","metadata":{}},{"cell_type":"code","source":"year2019 = amazon_dataset.loc[amazon_dataset[\"Year\"] == 2019]\n\nnp.round(year2019.shape[0] / 550, decimals=2) #books out in 2019 as a total amount of dataset\n","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:50.046124Z","iopub.execute_input":"2023-07-02T20:42:50.046378Z","iopub.status.idle":"2023-07-02T20:42:50.054080Z","shell.execute_reply.started":"2023-07-02T20:42:50.046355Z","shell.execute_reply":"2023-07-02T20:42:50.053142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That doesn't look right, ideally, that number would be .1, since we have 10 years of books and each year should get 10% representation in the dataset to be equal. But it is probably just a rounding error.\n\nTo be totally sure let's check that every year has equal weighting.","metadata":{}},{"cell_type":"code","source":"def yearRatios(dataset):\n    \"\"\"\n    gets the years out of a dataset and returns the ratio of that years entries \n    \n    #Parameters:\n    #    dataset (Pandas Dataframe): a dataset to take the years from\n\n    #Returns:\n    #    dictionary (dict): of both the year and the ratio of that year's contibution to all dataset values   \n\n    \"\"\"\n    years_dict = {}\n    year_series = dataset['Year']\n    total_instances = year_series.shape[0]\n    years = np.sort(year_series.unique())\n    for i in range(len(years)):\n        year = years[i]\n        total_year_amount = dataset.loc[dataset[\"Year\"] == year]\n        year_count = total_year_amount.shape[0]\n        year_ratio = np.round(year_count / total_instances, decimals=2)\n        years_dict[year] = year_ratio\n    return years_dict\n        \n    \nyearRatios(amazon_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:50.055562Z","iopub.execute_input":"2023-07-02T20:42:50.055964Z","iopub.status.idle":"2023-07-02T20:42:50.079084Z","shell.execute_reply.started":"2023-07-02T20:42:50.055934Z","shell.execute_reply":"2023-07-02T20:42:50.077890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thankfully it looks like all years are given equal weight, so nothing to worry about for stratification there. But let’s see if stratification is needed for the amount of fiction books vs. non-fiction books. ","metadata":{}},{"cell_type":"code","source":"Fiction_fraction = amazon_dataset.loc[amazon_dataset[\"Genre\"] == \"Fiction\"]\n\nnp.round(Fiction_fraction.shape[0] / 550, decimals=2) * 100","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:50.080154Z","iopub.execute_input":"2023-07-02T20:42:50.080528Z","iopub.status.idle":"2023-07-02T20:42:50.089576Z","shell.execute_reply.started":"2023-07-02T20:42:50.080501Z","shell.execute_reply":"2023-07-02T20:42:50.088173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems like there is a bias in favor of non-fiction books in the dataset, so the training and test sets will need to be stratified based on that.","metadata":{}},{"cell_type":"markdown","source":"# 2. Data cleaning function set up","metadata":{}},{"cell_type":"markdown","source":"First lets take out all books with a zero price, sadly my modeling assumption is I don't think Amazon is offering free products.","metadata":{}},{"cell_type":"code","source":"def noZeroPrice(dataset):\n    zero_indexes = dataset.loc[dataset[\"Price\"] == 0].index\n    return dataset.drop(zero_indexes)\n\nprice_transformer = FunctionTransformer(noZeroPrice)\n\namazon_no_zeros = price_transformer.transform(amazon_dataset)\n\namazon_no_zeros.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:50.093647Z","iopub.execute_input":"2023-07-02T20:42:50.094172Z","iopub.status.idle":"2023-07-02T20:42:50.105596Z","shell.execute_reply.started":"2023-07-02T20:42:50.094136Z","shell.execute_reply":"2023-07-02T20:42:50.104344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now dropping duplicate rows based on the book name. It looks like books were entered multiple times in different years. That might mess up model predictions.","metadata":{}},{"cell_type":"code","source":"def dropNameDupes(dataframe):\n    \"\"\"\n    finds the rows where the name for the book has been duplicated and drops them from the dataset\n    \n    #Parameters:\n    #    dataset (Pandas Dataframe): a dataset to take the names from\n\n    #Returns:\n    #    Pandas Dataframe: A new dataframe with the duplicate rows removed.\n    \n    \"\"\"\n\n    df = dataframe\n    return df[pd.DataFrame.duplicated(df,subset=[\"Name\"]) == False]\n\ndupes_transformer = FunctionTransformer(dropNameDupes)\n\namazon_nodupes_nozero = dupes_transformer.transform(amazon_no_zeros)\n\namazon_nodupes_nozero.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:50.108224Z","iopub.execute_input":"2023-07-02T20:42:50.108627Z","iopub.status.idle":"2023-07-02T20:42:50.120505Z","shell.execute_reply.started":"2023-07-02T20:42:50.108591Z","shell.execute_reply":"2023-07-02T20:42:50.119316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ouch, only 343 viable books to work with now, but it's what we got. \n\nLet’s put it all into a pipeline that cleans and standardizes the data. Also, that turns the genre catagorical feature into a one-hot encoded representation.","metadata":{}},{"cell_type":"code","source":"clean_pipline =  Pipeline([\n        ('no_zero_price', price_transformer),\n        ('no_dupes', dupes_transformer),\n    ])\n\ncleaned_set = clean_pipline.transform(amazon_dataset)\n\namazon_nums = amazon_dataset.select_dtypes(include=np.number).columns.tolist()\namazon_nums.remove(\"User Rating\")\ngenre_attrib = [\"Genre\"]\n\nfull_pipeline = ColumnTransformer([\n        ('std_scaler', StandardScaler(),amazon_nums),\n        (\"cat\", OneHotEncoder(), genre_attrib),\n    ])\n\n\namazon_ready = full_pipeline.fit_transform(cleaned_set)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:50.122627Z","iopub.execute_input":"2023-07-02T20:42:50.123407Z","iopub.status.idle":"2023-07-02T20:42:50.142195Z","shell.execute_reply.started":"2023-07-02T20:42:50.123374Z","shell.execute_reply":"2023-07-02T20:42:50.140839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the pipelines now work lets actually make the model.","metadata":{}},{"cell_type":"markdown","source":"# 3. Data preparation and model creation","metadata":{}},{"cell_type":"code","source":"\nclean_set = clean_pipline.transform(amazon_dataset)\n\namazon_train,amazon_test = train_test_split( clean_set , test_size = 0.2 , stratify= clean_set[\"Genre\"] )\n\namazon = amazon_train.drop(\"User Rating\", axis=1)\namazon_labels = amazon_train[\"User Rating\"].copy()\n\namazon_testset = amazon_test.drop(\"User Rating\", axis=1)\namazon_test_labels = amazon_test[\"User Rating\"].copy()\n\namazon_ready = full_pipeline.fit_transform(amazon)\n\namazon_test_ready = full_pipeline.transform(amazon_testset)\n\n\namazon_ready","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:50.144534Z","iopub.execute_input":"2023-07-02T20:42:50.145143Z","iopub.status.idle":"2023-07-02T20:42:50.170575Z","shell.execute_reply.started":"2023-07-02T20:42:50.145113Z","shell.execute_reply":"2023-07-02T20:42:50.169254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lin_reg = LinearRegression()\n\nlin_scores = cross_val_score(lin_reg, amazon_ready,amazon_labels,scoring=\"neg_mean_squared_error\",cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\n\npd.Series(lin_rmse_scores).describe()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:50.172051Z","iopub.execute_input":"2023-07-02T20:42:50.172422Z","iopub.status.idle":"2023-07-02T20:42:50.208089Z","shell.execute_reply.started":"2023-07-02T20:42:50.172387Z","shell.execute_reply":"2023-07-02T20:42:50.207228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mean error there isn't great, especially since in the exploratory part of the project most scores fell within the 4.5 - 4.8 range. So being off by at a minimum of .17~ is very big. Let’s try another model.","metadata":{}},{"cell_type":"code","source":"tree_reg = DecisionTreeRegressor()\n\ntree_scores = cross_val_score(tree_reg, amazon_ready,amazon_labels,scoring=\"neg_mean_squared_error\",cv=10)\n\ntree_rmse_scores = np.sqrt(-tree_scores)\n\npd.Series(tree_rmse_scores).describe()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:50.209440Z","iopub.execute_input":"2023-07-02T20:42:50.209827Z","iopub.status.idle":"2023-07-02T20:42:50.241247Z","shell.execute_reply.started":"2023-07-02T20:42:50.209788Z","shell.execute_reply":"2023-07-02T20:42:50.240142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm surprised to see the decision tree model work worse, I would have imagined an unrestrained tree would overfit but it’s performing worse than the linear regression model from before. Let’s see if it can be made better by changing some of the tree's hyperparameters.","metadata":{}},{"cell_type":"code","source":"parameter_search_grid = [\n    {'max_depth': [5, 15, 25], 'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson']},\n    {'max_depth': [5, 15, 25], 'max_features': [2, 3, 4]},\n  ]\nsearch_grid = GridSearchCV(tree_reg, parameter_search_grid , cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\nsearch_grid.fit(amazon_ready, amazon_labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:43:16.453510Z","iopub.execute_input":"2023-07-02T20:43:16.453893Z","iopub.status.idle":"2023-07-02T20:43:16.714151Z","shell.execute_reply.started":"2023-07-02T20:43:16.453864Z","shell.execute_reply":"2023-07-02T20:43:16.712873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(search_grid.best_estimator_)\nprint(search_grid.best_estimator_.feature_importances_)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:44:01.576926Z","iopub.execute_input":"2023-07-02T20:44:01.577251Z","iopub.status.idle":"2023-07-02T20:44:01.582654Z","shell.execute_reply.started":"2023-07-02T20:44:01.577225Z","shell.execute_reply":"2023-07-02T20:44:01.581611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A nice thing about grid search is that now we have the best hyperparameters for the decision tree model (of those that we tested), and we know what the most important features are, looking above user ratings are best predicted by reviews and year released. Which is surprising to me, I would have thought Genre would have a greater impact than year.","metadata":{}},{"cell_type":"code","source":"best_tree = DecisionTreeRegressor(max_features=2, max_depth=5)\nbest_tree_scores = cross_val_score(best_tree,amazon_ready,amazon_labels,scoring=\"neg_mean_squared_error\",cv=5)\n\nbest_tree_rmse_scores = np.sqrt(-best_tree_scores)\n\npd.Series(best_tree_rmse_scores).describe()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:43:42.043293Z","iopub.execute_input":"2023-07-02T20:43:42.043685Z","iopub.status.idle":"2023-07-02T20:43:42.065579Z","shell.execute_reply.started":"2023-07-02T20:43:42.043656Z","shell.execute_reply":"2023-07-02T20:43:42.064669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mean error went down, but a mean error around .27 is still far too high to be useful. One last thing I will try to achieve a good score is to apply gradient boosting and see if that does better.","metadata":{}},{"cell_type":"code","source":"gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\ngbrt_score = cross_val_score(gbrt,amazon_ready,amazon_labels,scoring=\"neg_mean_squared_error\",cv=5)\ngbrt_rmse_scores = np.sqrt(-gbrt_score)\n\npd.Series(gbrt_rmse_scores).describe()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:50.555525Z","iopub.execute_input":"2023-07-02T20:42:50.556615Z","iopub.status.idle":"2023-07-02T20:42:50.834861Z","shell.execute_reply.started":"2023-07-02T20:42:50.556575Z","shell.execute_reply":"2023-07-02T20:42:50.833194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mean error is better than both decision trees above, as expected from an ensemble model but linear regression still gives the lowest mean error, so let’s use that on the test set.","metadata":{}},{"cell_type":"code","source":"lin_reg.fit(amazon_ready,amazon_labels)\nlabels_pred = lin_reg.predict(amazon_test_ready)\nmse = mean_squared_error(amazon_test_labels,labels_pred)\ntest_rmse = np.sqrt(mse)\n\ntest_rmse","metadata":{"execution":{"iopub.status.busy":"2023-07-02T20:42:50.836353Z","iopub.execute_input":"2023-07-02T20:42:50.836655Z","iopub.status.idle":"2023-07-02T20:42:50.847207Z","shell.execute_reply.started":"2023-07-02T20:42:50.836634Z","shell.execute_reply":"2023-07-02T20:42:50.846164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test Root Mean Squared Error wasn't as high as I thought it was going to be. looking at the RSME estimator statistics of the cross-validation sets the test RMSE is in the 50~ quantile. So if you were to give the model a book’s reviews, price, year released, and genre (standardized on the backend) you would get a predicted user rating that would be roughly .19 off the real user rating. But again considering the range of the user rating variable .19 is relatively large.","metadata":{}},{"cell_type":"markdown","source":"# 4. Project Review","metadata":{}},{"cell_type":"markdown","source":"## Why I chose this dataset\n\nIt was always going to be an uphill battle. The dataset is limited to say the least, but that is also why I chose it as my starter project. Amazon book reviews are an easy-to-understand and approachable subject. The dataset was small enough to where I didn't have any fear of running multiple different test functions on the whole dataset since there were only 550 instances. It also had a clear and easy target variable for a dataset, I thought that for authors it would be nice to have a model that could predict their book’s user score with a few input features. Small dataset + defined goal + easily understandable data made this a nice dataset to start off on.\n\n## Limitations\n\nGiven 550 instances and a few features to work with will hard limit you in pretty much any instance I think. Exploring the data and finding out that there were so many duplicates that it took out roughly 35% of the dataset when they were removed was a big blow.\n\nEven though I really didn't have high hopes for any model to perform well on the dataset. A lot of the reason books are rated as they are is due to the broader culture surrounding them, which for any dataset would be hard to model, especially with such limited features. The highest-rated books have genres that covered young adult novels, religious works, biographies, and more. All those genres are crudely pigeonholed into \"fiction\" or \"non-fiction\". But you work with the data you have, and I chose this to start with.\n\n## Errors\n\nMultiple times I made functions that ended up not helping and I cut them out. One was a standardizer before I figured out how Sklearns pipelining worked. Another was a function that standardized the numerical variables and then put them back in the dataframe, again not really needed since I was going to have to drop the book name and author feature since those would blow up the dimensionality if I were to one-hot encode them. \n\nDon't get me wrong, authors and book titles will impact the user rating of a book. But it goes back to the structural problem of the limited data. If the model had enough data to encode pop culture author name and book title would be a helpful feature. But I thought it was facile here with already so few features and instances.\n\nBut these errors did help me learn more about the tools of Sklearn and Pandas, so I'm not upset about having made them.\n\n## For future projects\n\n* Try out imputing missing data instead of dropping it. Or make a Sklearn class function to test if imputing or not performs better for the model.\n\n* Instead of creating Python functions and then putting them through a function transformer to make them work with pipelining, just make a Sklearn class for it.\n\n* Create a more detailed project outline next time so you don't make pointless functions.\n\n* Try to make new features by combining features and seeing if that helps.\n\n* Test out more models/model combinations.\n\nOverall I'm happy with how the project went, I just wanted to get my hands dirty and go through a whole project and I did. I'm putting this out as my starting place, I hope to get only better from here. \n\n\nIf you have any comments on what I could do better, feel free to add them!","metadata":{}}]}